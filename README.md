Durante el desarrollo del proyecto combinamos el análisis visual con un modelado que nos ayudó a entender qué variables tenían más peso en la predicción del Churn. El archivo principal que usamos para mostrar los resultados fue el Visualizaciones.ipynb, que reúne los gráficos 
que fuimos generando y que nos permitieron interpretar el comportamiento de los clientes de una manera más clara. La idea era ponernos en el rol de un equipo de datos de una empresa real y analizar qué señales muestran los clientes antes de dejar de comprar.No nos quedamos solamente con los gráficos, sino que también probamos un modelo
para ver si esas señales podían aprenderse de manera automática, y además utilizamos MLflow para registrar todo el proceso como si estuviéramos trabajando en un flujo profesional. Primero definimos la variable churn, que fue una de las decisiones clave del trabajo. 
Después de analizar los patrones de compra y discutir varias posibilidades, decidimos que un cliente se considera churn si pasan 180 días sin que haga una nueva compra. Elegimos este horizonte porque recomendación del profesor que nos pareció razonable teniendo en cuenta la variabilidad en
la frecuencia de compra de los usuarios. Algunos compran seguido y otros no, por lo que un período muy corto iba a marcar como churn a clientes normales, y uno demasiado largo iba a perder sentido práctico. Con la regla de 180 días logramos una buena separación entre clientes
activos y clientes realmente inactivos. Antes de llegar al análisis visual, pasamos un buen rato limpiando los datos. Los archivos originales de clientes, productos y órdenes venían con distintos problemas: fechas en formatos mezclados, columnas con nombres poco consistentes,
montos escritos con símbolos y comas y algunos duplicados. Para que el análisis fuera confiable, normalizamos nombres, convertimos fechas, reconstruimos montos en valores numéricos y unimos correctamente las tablas. En concreto, dentro de los CSV aplicamos varias
transformaciones importantes: en clients_clean.csv revisamos identificadores duplicados, corregimos inconsistencias en los campos de fecha de registro y estandarizamos los nombres y formatos de las columnas. En products_clean.csv limpiamos los precios eliminando símbolos, 
convertimos los montos a formato numérico, revisamos códigos de producto mal cargados y unificamos categorías. En orders_clean.csv reconstruimos correctamente las fechas de cada orden, pasamos todos los importes a valores numéricos sin caracteres extraños, eliminamos filas duplicadas y validamos que
todos los IDs de cliente y producto existieran en sus tablas correspondientes. Esta limpieza fue clave para garantizar que, al unir las tres fuentes, no aparecieran errores o relaciones inválidas. 
Esta parte no se ve tanto en el notebook final, pero sin ese trabajo previo sería imposible ver patrones reales o entrenar un modelo mínimamente estable.

Con los datos ya en buen estado, armamos un dataset analítico con variables que resumen el comportamiento de cada cliente. Calculamos la recencia, la frecuencia total de compras, el monto total gastado, el promedio por orden y algunos otros indicadores que
sirvieron para entender mejor la actividad de cada usuario. En Visualizaciones.ipynb usamos estas variables para generar gráficos comparativos entre clientes churn y no churn. Lo interesante fue que muchas de las diferencias se
notaban a simple vista: los clientes Churn tenían, en general, recencias mucho más altas, menos compras en total y montos más bajos. También generamos gráficos de correlación y algunas visualizaciones temporales que mostraban cómo
la actividad general de la plataforma cambiaba mes a mes. Además de las visualizaciones, quisimos probar un modelo de Random Forest para evaluar si estas variables realmente contenían información predictiva útil. Elegimos este modelo porque suele 
funcionar bien sin necesidad de un trabajo muy complejo de ajuste y también porque nos permite ver la importancia relativa de cada feature. Los resultados fueron coherentes con lo que ya se veía en los gráficos: la recencia, la frecuencia y el
monto total gastado fueron las variables más importantes. Esto nos dio más confianza en que las señales visuales que encontramos también se reflejaban en el comportamiento del modelo. Para manejar esta parte del modelado de manera más organizada, usamos MLflow.
Esta herramienta nos permitió registrar los experimentos, guardar las métricas y también los modelos entrenados. El proceso no fue sencillo: después de correr el Random Forest en el notebook, configuramos MLflow para que creara automáticamente una carpeta
de seguimiento y abriera su interfaz local. En nuestro caso ejecutamos el comando para iniciar el servidor de MLflow y luego entramos a la página que aparece normalmente en la dirección local, la típica URL que se abre en el navegador con el panel de experimentos. 
Ahí se podía ver cada corrida que hicimos del modelo, mostrando las métricas como accuracy, f1, roc_auc y otras, además de los parámetros del Random Forest como la cantidad de árboles y el random_state. La interfaz también mostraba gráficos simples de comparación 
entre experimentos y permitía descargar el modelo que se había guardado. En resumen, MLflow nos sirvió para tener un registro ordenado de lo que probamos, algo que en un proyecto real es clave para trabajar en equipo y para no perder los resultados de cada corrida.
Después de todo el análisis, las conclusiones fueron bastante claras. El churn no aparece de golpe, sino que se puede anticipar mirando el comportamiento reciente del cliente. Los usuarios que terminan abandonando suelen tener recencias cada vez más largas,
una frecuencia baja y un gasto reducido. Estas señales se veían claramente en los gráficos y también en la importancia de las variables del Random Forest. El uso de MLflow nos permitió confirmar y guardar estas observaciones de forma más profesional, y el notebook de 
visualizaciones refleja de manera clara lo más importante del comportamiento de los clientes.

En general, el proyecto nos permitió pasar por todo el proceso de un análisis aplicado: limpiar datos, construir variables, visualizar, modelar y registrar los experimentos. Aunque no nos enfocamos tanto en optimizar el modelo, el objetivo principal de 
entender el churn y lograr una representación visual clara se cumplió. El trabajo final muestra de forma entendible qué características tienen los clientes que abandonan y cómo se pueden detectar estos patrones antes de que ocurra el churn.
