Francisco Benavides, Agustín Rivero y Juan Manuel Silveira


Durante el desarrollo del proyecto, combinamos el análisis visual con un modelado que nos ayudó a entender qué variables tenían más peso en la predicción del Churn. El archivo principal que usamos para mostrar los resultados fue Visualizaciones.ipynb, que reúne los gráficos que fuimos generando y que nos permitieron interpretar el comportamiento de los clientes de una manera más clara. La idea era ponernos en el rol de un equipo de datos de una empresa real y analizar qué señales muestran los clientes antes de dejar de comprar. No nos quedamos solamente con los gráficos, sino que también probamos un modelo para ver si esas señales podían aprenderse de manera automática, y además utilizamos MLflow para registrar todo el proceso como si estuviéramos trabajando en un flujo profesional.
Primero definimos la variable churn, que fue una de las decisiones clave del trabajo. Después de analizar los patrones de compra y discutir varias posibilidades, que un cliente se considera churn si pasan 180 días sin que haga una nueva compra. Elegimos este horizonte porque fue el que nos recomendó el profesor y consideramos adecuado. Algunos compran seguido y otros no, por lo que un período muy corto iba a marcar como churn a clientes normales, y uno demasiado largo iba a perder sentido práctico. Con la regla de 180 días logramos una buena separación entre clientes activos y clientes realmente inactivos.

Antes del análisis visual, limpiamos los datos. Los archivos originales de clientes, productos y órdenes venían con distintos problemas: fechas en formatos mezclados, columnas con nombres poco consistentes, montos escritos con símbolos y comas, valores faltantes y algunos duplicados. Si queriamos que el análisis pudiese ser visto como confiable,  tuvimos que normalizar nombres, convertir fechas y unir correctamente tablas. Dentro de los CSV aplicamos varias transformaciones importantes, por ejemplo, en clients_clean.csv revisamos identificadores duplicados, corregimos inconsistencias en los campos de fecha de registro y estandarizamos nombres y formatos de las columnas. En orders_clean.csv reconstruimos correctamente las fechas de cada orden, pasamos todos los importes a valores numéricos sin caracteres extraños, eliminamos filas duplicadas y validamos que todos los IDs de cliente y producto existieran en sus tablas correspondientes. 

La limpieza fue clave para garantizar que, al unir las fuentes, no aparecieran errores o relaciones inválidas. Esta parte no se ve tanto en el notebook final, pero sin tal trabajo previo sería imposible ver patrones reales o entrenar un modelo estable.

Con los datos ya en un buen estado, armamos un dataset analítico con variables que resumen el comportamiento de cada cliente. Calculamos la recencia, la frecuencia total de compras, el monto total gastado, el promedio por orden y algunos otros indicadores que sirvieron para entender mejor la actividad de cada usuario. En Visualizaciones.ipynb usamos estas variables para generar gráficos comparativos entre clientes churn y no churn. Lo interesante fue que muchas de las diferencias se notaban a simple vista, los clientes churn tenían recencias mucho más altas, menos compras en total y montos más bajos. También generamos gráficos de correlación y algunas visualizaciones temporales que mostraban cómo la actividad general de la plataforma cambiaba mes a mes.

Además del análisis visual, probamos distintos modelos predictivos con el fin de poder evaluar si efectivamente la información del comportamiento contenía patrones aprendibles. Comenzamos con una regresión logística y un árbol de decisión como modelos base, y luego utilizamos un Random Forest como modelo principal. Elegimos Random Forest porque suele ser un modelo robusto, capaz de capturar relaciones no lineales sin requerir tuning complejo, y porque permite analizar la importancia relativa de las variables. Los resultados fueron coherentes con lo observado gráficamente: la recencia, la frecuencia y el monto total gastado fueron las variables más influyentes. Esto reforzó la idea de que los patrones visuales realmente reflejaban comportamientos consistentes en los datos.

Para manejar la parte del modelado de forma más organizada, integramos MLflow en nuestro flujo de trabajo. Configuramos un experimento, registramos parámetros, métricas y modelos, y visualizamos las corridas en la interfaz gráfica. MLflow nos permitió guardar cada ejecución del modelo, comparar métricas como accuracy, f1 y roc_auc, y mantener un registro ordenado de los parámetros del Random Forest, como la cantidad de árboles y el random_state. Esto nos acercó a un trabajo más profesional.
En cuanto a la colaboración, aunque el proyecto se ejecutó y desarrolló finalmente en una misma computadora, durante el proceso compartimos los bloques de código por WhatsApp por una cuestión de comodidad y velocidad para revisarlos entre nosotros. Estos fragmentos ya habían sido probados previamente en Google Colab, lo que permitió verificar que funcionaran antes de incorporarlos al proyecto local. De esta manera mantuvimos una dinámica de trabajo fluida sin necesidad de sincronizar varios entornos, pero asegurándonos siempre de que cada parte del código estuviera testeada antes de integrarse.

Después de todo el análisis, las conclusiones fueron bastante claras. El churn no aparece de golpe, sino que se puede anticipar observando el comportamiento reciente del cliente. Los usuarios que terminan abandonando suelen mostrar recencias cada vez más largas, una frecuencia baja y un gasto reducido. Estas señales aparecieron de forma consistente tanto en los gráficos como en la importancia de las variables del modelo. El uso de MLflow permitió registrar estas observaciones y las pruebas realizadas de manera profesional, mientras que el notebook de visualizaciones funcionó como una forma clara y accesible de comunicar los patrones encontrados.
El proyecto nos permitió recorrer todas las etapas de un análisis aplicado desde la limpieza de datos hasta el  modelado, la experimentación y la documentación. Si bien no buscamos optimizar el modelo al máximo, cumplimos el objetivo principal de entender el churn y representarlo de manera visual y analítica. El trabajo final muestra qué características distinguen a los clientes que abandonan y cómo estos patrones pueden detectarse antes de que ocurra el churn.

